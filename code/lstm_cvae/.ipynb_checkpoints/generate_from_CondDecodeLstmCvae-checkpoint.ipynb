{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import cPickle as pickle\n",
    "import csv\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.models import Model, load_model\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from data_generator import DataGenerator\n",
    "from lstm_cvae_model import ModelConfig\n",
    "\n",
    "def get_sample_config():\n",
    "    sample_config = {\n",
    "        \"model_dir\": \"/Users/tongwang/Playground/deepjoke/code/model_checkpoints/lstm_cvae/20170618_072219\",\n",
    "        \"starter_sentences\": [\"a sexy\", \"what\", \"why\", \"i have a dream\", \"once upon a time\", \"trump\"],\n",
    "        \"temperatures\": [None, 0.2, 0.5, 1.0, 1.5],\n",
    "        \"scores\": [0, 1, 5, 10, 20],\n",
    "        \"num\": 5\n",
    "    }\n",
    "    return sample_config\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum()\n",
    "\n",
    "def sample(preds, temperature=None):\n",
    "    \"\"\"Helper function to sample an index from a probability array; if temperature is None, \n",
    "    then sample greedily\"\"\"\n",
    "    if temperature is None:\n",
    "        return np.argmax(preds)\n",
    "    else:\n",
    "        preds = np.asarray(preds).astype('float64')\n",
    "        preds = softmax(preds)  # Convert logits into probabilities\n",
    "        preds = np.log(preds) / temperature\n",
    "        exp_preds = np.exp(preds)\n",
    "        preds = exp_preds / np.sum(exp_preds)\n",
    "        probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)\n",
    "\n",
    "def tokens_to_words(tokens, tokenizer, eos=\"\"):\n",
    "    \"\"\"Helper function to turn an 1-d array of tokens tokenized by tokenizer back to words\"\"\"\n",
    "    reverse_word_index = {index: word for word, index in tokenizer.word_index.iteritems()}\n",
    "    reverse_word_index[0] = eos\n",
    "    words = [reverse_word_index.get(token) for token in tokens]\n",
    "    text = \" \".join(words)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/lib/python2.7/site-packages/keras/models.py:258: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "  warnings.warn('No training configuration found in save file: '\n"
     ]
    }
   ],
   "source": [
    "model_dir = \"/Users/tongwang/Playground/deepjoke/code/model_checkpoints/lstm_cvae/20170618_072219\"\n",
    "encoder_path = model_dir + \"/encoder_checkpoint\"\n",
    "generator_path = model_dir + \"/generator_checkpoint\"\n",
    "tokenizer_path = model_dir + \"/tokenizer.p\"\n",
    "model_config_path = model_dir + \"/model_config.p\"\n",
    "\n",
    "encoder = load_model(encoder_path)\n",
    "generator = load_model(generator_path)\n",
    "tokenizer = pickle.load(open(tokenizer_path, \"r\"))\n",
    "model_config = pickle.load(open(model_config_path, \"r\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generate_text(target_score, generator, model_config, tokenizer, \n",
    "                  starter_sentence=\"\", temperature=None, variation=1.0, eos=\"\"):\n",
    "    \"\"\"Function to generate paragraphs given a target score, a random latent vector,\n",
    "    and (optionally) a starter sentence.\n",
    "    \n",
    "    Args:\n",
    "        -target_score\n",
    "        -generator\n",
    "        -model_config\n",
    "        -tokenizer\n",
    "        -temperature: if None, generate text greedily; otherwise sample stochastically\n",
    "    Returns:\n",
    "        -model_config.batch_size many pieces of text\n",
    "    \"\"\"\n",
    "    # Prepare inputs\n",
    "    z = np.random.normal(scale=variation., size=(model_config.batch_size, model_config.latent_size))\n",
    "    print(z)\n",
    "    scores = np.repeat(target_score, model_config.batch_size)\n",
    "    cur_sentence = [starter_sentence]\n",
    "    cur_sequence = tokenizer.texts_to_sequences(cur_sentence)\n",
    "    cur_sequence = pad_sequences(cur_sequence, maxlen=model_config.max_sequence_length,\n",
    "                                 padding='post', truncating='post')\n",
    "    cur_sequence = np.repeat(cur_sequence, model_config.batch_size, axis=0)\n",
    "    \n",
    "    reverse_word_index = {index: word for word, index in tokenizer.word_index.iteritems()}\n",
    "\n",
    "    # Iteratively predict the next word\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        true_len = len(cur_sequence[0][cur_sequence[0]>0])\n",
    "        if true_len == model_config.max_sequence_length:\n",
    "            break\n",
    "        next_preds = generator.predict([cur_sequence, scores, z])[0, true_len-1, :] # predicted next word\n",
    "        next_token = sample(next_preds, temperature)\n",
    "        if next_token == 0:\n",
    "            break\n",
    "        cur_sequence[0][true_len] = next_token\"\"\"\n",
    "    while True:\n",
    "        true_lens = np.sum(cur_sequence > 0, axis=1)\n",
    "        last_tokens = np.array([cur_sequence[i, true_lens[i]-1] for i in range(model_config.batch_size)])\n",
    "        print(true_lens)\n",
    "        print(last_tokens)\n",
    "        if np.min(np.logical_or(\n",
    "            (true_lens == model_config.max_sequence_length),\n",
    "            (last_tokens == 0))) > 0:\n",
    "            break\n",
    "        all_preds = generator.predict([cur_sequence, scores, z])\n",
    "        for i in range(model_config.batch_size):\n",
    "            if last_tokens[i] == 0:\n",
    "                continue\n",
    "            else:\n",
    "                next_preds = all_preds[i, true_lens[i]-1, :]\n",
    "                next_token = sample(next_preds, temperature)\n",
    "                cur_sequence[i, true_lens[i]] = next_token\n",
    "\n",
    "    pred_texts = []\n",
    "    for i in range(model_config.batch_size):\n",
    "        pred_sequence = cur_sequence[i][cur_sequence[i]>0]\n",
    "        pred_text = tokens_to_words(pred_sequence, tokenizer=tokenizer, eos=eos)\n",
    "        pred_texts.append(pred_text)\n",
    "    \n",
    "    return pred_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ -9.15379304   8.27983002   4.92072988 ...,  -1.42404489 -16.31653623\n",
      "   -6.78862419]\n",
      " [ 11.10850578   4.5269289    3.31255394 ..., -18.58796288 -11.939461\n",
      "   -6.2921765 ]\n",
      " [ -4.67132108   4.68724232   8.91312393 ...,  -1.34071219 -14.39342259\n",
      "    4.81227496]\n",
      " ..., \n",
      " [-19.62167796  -7.77455219  -8.27733414 ...,  16.73833075   8.09323821\n",
      "  -17.93385031]\n",
      " [  6.18136546   7.4858186   -7.84384357 ..., -16.35000095  -3.1640482\n",
      "   -5.57649955]\n",
      " [  7.83658104  -2.37994521 -13.66690602 ...,  -7.76565026   4.85707867\n",
      "    1.38275973]]\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "[4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4]\n",
      "[2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2]\n",
      "[  23    4   23   23 1904 1309    8    1 1311   23  101   23   23   23   23\n",
      "  101 1856   23    4 1856   23   23   23   23   23   23    1   23  171   23\n",
      "   23   23]\n",
      "[3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3]\n",
      "[ 108 1169   13    5 3681 2276  575 1856    5   13 2531    5   22    3   16\n",
      "    4  434    3 1017    6    3   72   22   22  108  108   11   13  787  108\n",
      "   13  636]\n"
     ]
    }
   ],
   "source": [
    "s = generate_text(generator=generator, model_config=model_config, tokenizer=tokenizer,\n",
    "              target_score=2, starter_sentence=\"a\", variation=3.0, temperature=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = np.random.normal(size=(5, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.68480836,  0.34998149,  0.38515765, -0.41151523],\n",
       "       [ 1.52700636,  0.66575251,  0.31049105,  0.18926559],\n",
       "       [ 0.93849434,  0.55826684, -0.61776509,  0.21317279],\n",
       "       [-0.37012515, -1.25836485,  0.45113172,  0.88775646],\n",
       "       [-0.03215376, -0.3352601 ,  0.82262767, -0.28751208]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
