{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#TODO\n",
    "#-Split the KL loss term and Recon loss term\n",
    "\n",
    "from __future__ import print_function\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.layers import Dense, LSTM, Embedding, Input, RepeatVector, Lambda\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers.merge import concatenate\n",
    "from keras import optimizers\n",
    "from data_generator import DataGenerator\n",
    "\n",
    "class ModelConfig():\n",
    "    \"\"\"Class to hold all model configs; required to instantiate a LSTM CVAE model\"\"\"\n",
    "    def __init__(self,\n",
    "        input_path,\n",
    "        embedding_path,\n",
    "        model_dir,\n",
    "        embedding_dim=100,\n",
    "        batch_size=32,\n",
    "        epochs=5,\n",
    "        min_score=0,\n",
    "        max_nb_words=50000,\n",
    "        max_nb_examples=None,\n",
    "        max_sequence_length=300,\n",
    "        lstm_size_encoder=256,\n",
    "        lstm_size_decoder=256,\n",
    "        intermediate_size=128,\n",
    "        latent_size=64,\n",
    "        kl_weight=1.,\n",
    "        optimizer=\"adam\",\n",
    "        validation_split=0.1):\n",
    "        self.input_path = input_path\n",
    "        self.embedding_path = embedding_path\n",
    "        self.model_dir = model_dir\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.epochs = epochs\n",
    "        self.min_score = min_score\n",
    "        self.max_nb_words = max_nb_words\n",
    "        self.max_nb_examples = max_nb_examples\n",
    "        self.max_sequence_length = max_sequence_length\n",
    "        self.lstm_size_encoder = lstm_size_encoder\n",
    "        self.lstm_size_decoder = lstm_size_decoder\n",
    "        self.intermediate_size = intermediate_size\n",
    "        self.latent_size = latent_size\n",
    "        self.kl_weight = kl_weight\n",
    "        self.optimizer = optimizer\n",
    "        self.validation_split = validation_split\n",
    "\n",
    "class UncondDecodeLstmCvae(object):\n",
    "    \"\"\"Class to hold LSTM CVAE model\n",
    "    This implementation users the repeated code z as the only input to the decoder \n",
    "    (i.e. unconditional decoder)\n",
    "    \"\"\"\n",
    "    def __init__(self, model_config, tokenizer):\n",
    "        self.config = model_config\n",
    "        self.config.word_index = tokenizer.word_index\n",
    "        self.config.num_words = min(model_config.max_nb_words, \n",
    "                                    len(tokenizer.word_index))\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def load_embedding(self):\n",
    "        \"\"\"Load and prepare embedding matrix\"\"\"\n",
    "        embeddings_index = {}\n",
    "        f = open(self.config.embedding_path)\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = coefs\n",
    "        f.close()\n",
    "        embedding_matrix = np.zeros((self.config.num_words + 1, self.config.embedding_dim))\n",
    "        for word, i in self.config.word_index.items():\n",
    "            if i >= self.config.max_nb_words:\n",
    "                continue\n",
    "            embedding_vector = embeddings_index.get(word)\n",
    "            if embedding_vector is not None:\n",
    "                # words not found in embedding index will be all-zeros.\n",
    "                embedding_matrix[i] = embedding_vector\n",
    "        return embedding_matrix\n",
    "    \n",
    "    def build(self):\n",
    "        \"\"\"Construct lstm cvae model\"\"\"\n",
    "        # Load embedding in Embedding layer\n",
    "        embedding_matrix = self.load_embedding()\n",
    "        embedding_layer = Embedding(self.config.num_words + 1,\n",
    "                                    self.config.embedding_dim,\n",
    "                                    weights=[embedding_matrix],\n",
    "                                    input_length=self.config.max_sequence_length,\n",
    "                                    trainable=False)\n",
    "        \n",
    "        # Q(z|X,y) -- encoder\n",
    "        # embedded sequence input\n",
    "        sequence_inputs = Input(batch_shape=(self.config.batch_size, self.config.max_sequence_length), dtype='int32')\n",
    "        embedded_inputs = embedding_layer(sequence_inputs)\n",
    "        x = LSTM(self.config.lstm_size_encoder, return_sequences=False)(embedded_inputs)\n",
    "        score_inputs = Input(batch_shape=(self.config.batch_size, 1))\n",
    "        x_joint = concatenate([x, score_inputs], axis=1)\n",
    "        x_encoded = Dense(self.config.intermediate_size, activation='tanh')(x_joint)\n",
    "        z_mean = Dense(self.config.latent_size)(x_encoded)\n",
    "        z_log_sigma = Dense(self.config.latent_size)(x_encoded)\n",
    "\n",
    "        # Sample z ~ Q(z|X,y)\n",
    "        def sampling(args):\n",
    "            z_mean, z_log_sigma = args\n",
    "            epsilon = K.random_normal(shape=(self.config.batch_size, self.config.latent_size), \n",
    "                                       mean=0., stddev=1.)            \n",
    "            return z_mean + K.exp(z_log_sigma/2.) * epsilon\n",
    "        \n",
    "        z = Lambda(sampling)([z_mean, z_log_sigma])\n",
    "        z_cond = concatenate([z, score_inputs], axis=1)\n",
    "\n",
    "        # P(X|z,y) -- decoder\n",
    "        z_repeated = RepeatVector(self.config.max_sequence_length)(z_cond)\n",
    "        \n",
    "        decoder_h = LSTM(self.config.lstm_size_decoder, return_sequences=True)\n",
    "        decoder_out = Dense(self.config.num_words + 1)\n",
    "        \n",
    "        h_decoded = decoder_h(z_repeated)\n",
    "        x_decoded = decoder_out(h_decoded)\n",
    "        # Construct three models\n",
    "        # vae\n",
    "        vae = Model([sequence_inputs, score_inputs], x_decoded)\n",
    "        # encoder\n",
    "        encoder = Model([sequence_inputs, score_inputs], z_mean)\n",
    "        # generator\n",
    "        generator_z_inputs = Input(batch_shape=(self.config.batch_size, self.config.latent_size))\n",
    "        generator_z_cond = concatenate([generator_z_inputs, score_inputs], axis=1)\n",
    "        generator_z_repeated = RepeatVector(self.config.max_sequence_length)(generator_z_cond)\n",
    "        generator_h_decoded = decoder_h(generator_z_repeated)\n",
    "        generator_x_decoded = decoder_out(generator_h_decoded)\n",
    "        generator = Model([generator_z_inputs, score_inputs], generator_x_decoded)\n",
    "\n",
    "\n",
    "        def vae_loss(y_true, y_pred):\n",
    "            \"\"\" Calculate loss = reconstruction loss + KL loss for each data in minibatch \"\"\"\n",
    "            # E[log P(X|z,y)]\n",
    "            recon = K.sum(K.sparse_categorical_crossentropy(\n",
    "                output=y_pred, target=y_true, from_logits=True), axis=1)\n",
    "            # D_KL(Q(z|X,y) || P(z|X)); calculate in closed form as both dist. are Gaussian\n",
    "            kl = 0.5 * K.sum(K.exp(z_log_sigma) + K.square(z_mean) - 1. - z_log_sigma, axis=1)\n",
    "            return recon + kl\n",
    "        \n",
    "        vae.compile(loss=vae_loss, optimizer=self.config.optimizer)\n",
    "        \n",
    "        self.vae = vae\n",
    "        self.encoder = encoder\n",
    "        self.generator = generator\n",
    "        \n",
    "    def fit(self, x_train, y_s_train, x_val, y_s_val):\n",
    "        \"\"\"Fit vae model, and store ouput models, configs and associated tokenizer\"\"\"\n",
    "        # Cut training and validation sets to multiples of batch_size\n",
    "        train_cap = int(np.floor(x_train.shape[0] / self.config.batch_size) * self.config.batch_size)\n",
    "        val_cap = int(np.floor(x_val.shape[0] / self.config.batch_size) * self.config.batch_size)\n",
    "        x_train = x_train[0:train_cap, :]\n",
    "        y_s_train = y_s_train[0:train_cap]\n",
    "        x_val = x_val[0:val_cap, :]\n",
    "        y_s_val = y_s_val[0:val_cap]\n",
    "        # Reshape a version of x as targets\n",
    "        x_train_reshaped = x_train.reshape(x_train.shape[0], x_train.shape[1], 1)\n",
    "        x_val_reshaped = x_val.reshape(x_val.shape[0], x_val.shape[1], 1)\n",
    "        # Fit \n",
    "        self.vae.fit([x_train, y_s_train], x_train_reshaped,\n",
    "                     batch_size=self.config.batch_size,\n",
    "                     epochs=self.config.epochs,\n",
    "                     validation_data=([x_val, y_s_val], x_val_reshaped))\n",
    "        # Save outputs\n",
    "        try:\n",
    "            os.makedirs(self.config.model_dir)\n",
    "        except:\n",
    "            print(\"Did not make model dir\")\n",
    "        \n",
    "        import gc; gc.collect()\n",
    "        \n",
    "        #self.vae.save(self.config.model_dir + \"/vae_checkpoint\")\n",
    "        self.encoder.save(self.config.model_dir + \"/encoder_checkpoint\")\n",
    "        self.generator.save(self.config.model_dir + \"/generator_checkpoint\")\n",
    "        pickle.dump(self.config, open(self.config.model_dir + \"/model_config.p\", \"wb\" ))\n",
    "        pickle.dump(self.tokenizer, open(self.config.model_dir + \"/tokenizer.p\", \"wb\" ))\n",
    "\n",
    "def test_UncondDecodeLstmCvae():\n",
    "    \"\"\"Function to test UncondDecodeLstmCvae class\"\"\"\n",
    "    # Construct an ModelConfig object\n",
    "    BASE_DIR = os.getcwd().replace(\"/lstm_cvae\", \"\")\n",
    "    GLOVE_DIR = BASE_DIR.replace(\"/code\", \"/glove.6B/\")\n",
    "    EMBEDDING_PATH=os.path.join(GLOVE_DIR, 'glove.6B.100d.txt')\n",
    "    TEXT_DATA_DIR = os.path.join(BASE_DIR.replace(\"/code\", '/joke-dataset/'), \"reddit_jokes.json\")\n",
    "    MODEL_DIR = BASE_DIR + \"/model_checkpoints/lstm_cvae/test/{:%Y%m%d_%H%M%S}\".format(datetime.now())\n",
    "    model_config = ModelConfig(input_path=TEXT_DATA_DIR,\n",
    "                           embedding_path=EMBEDDING_PATH,\n",
    "                           model_dir=MODEL_DIR,\n",
    "                           epochs=2,\n",
    "                           max_nb_examples=1000,\n",
    "                           max_sequence_length=100,\n",
    "                           batch_size=32,\n",
    "                           optimizer=\"RMSprop\")\n",
    "\n",
    "    # Generate data\n",
    "    data_generator = DataGenerator(\n",
    "        input_path=model_config.input_path,\n",
    "        min_score=model_config.min_score, \n",
    "        max_nb_words=model_config.max_nb_words,\n",
    "        max_nb_examples=model_config.max_nb_examples,\n",
    "        max_sequence_length=model_config.max_sequence_length,\n",
    "        validation_split=model_config.validation_split)\n",
    "    x_train, y_l_train, y_s_train, x_val, y_l_val, y_s_val, tokenizer = data_generator.generate()\n",
    "\n",
    "    # Build and fit model\n",
    "    cvae = UncondDecodeLstmCvae(model_config, tokenizer)\n",
    "    cvae.build()\n",
    "    cvae.fit(x_train, y_s_train, x_val, y_s_val)\n",
    "    print(\"Test passed! Go check model_dir for outputs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read in 194553 jokes.\n",
      "Read in 194553 scores.\n",
      "\n",
      "Found 5853 unique words; using 5853 unique words\n",
      "\n",
      "Shape of training features: (900, 100)\n",
      "Shape of training language model labels: (900, 100, 1)\n",
      "Shape of training score labels: (900,)\n",
      "Shape of validation features: (100, 100)\n",
      "Shape of validation language model labels: (100, 100, 1)\n",
      "Shape of validation score labels: (100,)\n",
      "Train on 896 samples, validate on 96 samples\n",
      "Epoch 1/2\n",
      "896/896 [==============================] - 27s - loss: 696.2824 - val_loss: 514.7416\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 2/2\n",
      "896/896 [==============================] - 24s - loss: 405.1683 - val_loss: 347.2717\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Test passed! Go check model_dir for outputs\n"
     ]
    }
   ],
   "source": [
    "test_UncondDecodeLstmCvae()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    # Generate data\\ndata_generator = DataGenerator(\\n        input_path=model_config.input_path,\\n        min_score=model_config.min_score, \\n        max_nb_words=model_config.max_nb_words,\\n        max_nb_examples=model_config.max_nb_examples,\\n        max_sequence_length=model_config.max_sequence_length,\\n        validation_split=model_config.validation_split)\\nx_train, y_l_train, y_s_train, x_val, y_l_val, y_s_val, tokenizer = data_generator.generate()'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BASE_DIR = os.getcwd().replace(\"/lstm_cvae\", \"\")\n",
    "GLOVE_DIR = BASE_DIR.replace(\"/code\", \"/glove.6B/\")\n",
    "EMBEDDING_PATH=os.path.join(GLOVE_DIR, 'glove.6B.100d.txt')\n",
    "TEXT_DATA_DIR = os.path.join(BASE_DIR.replace(\"/code\", '/joke-dataset/'), \"reddit_jokes.json\")\n",
    "MODEL_DIR = BASE_DIR + \"/model_checkpoints/lstm_cvae/test/{:%Y%m%d_%H%M%S}\".format(datetime.now())\n",
    "model_config = ModelConfig(input_path=TEXT_DATA_DIR,\n",
    "                           embedding_path=EMBEDDING_PATH,\n",
    "                           model_dir=MODEL_DIR,\n",
    "                           epochs=2,\n",
    "                           max_nb_examples=1000,\n",
    "                           max_sequence_length=100,\n",
    "                           batch_size=32,\n",
    "                           optimizer=\"RMSprop\")\n",
    "\n",
    "\n",
    "    # Generate data\n",
    "data_generator = DataGenerator(\n",
    "        input_path=model_config.input_path,\n",
    "        min_score=model_config.min_score, \n",
    "        max_nb_words=model_config.max_nb_words,\n",
    "        max_nb_examples=model_config.max_nb_examples,\n",
    "        max_sequence_length=model_config.max_sequence_length,\n",
    "        validation_split=model_config.validation_split)\n",
    "x_train, y_l_train, y_s_train, x_val, y_l_val, y_s_val, tokenizer = data_generator.generate()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
