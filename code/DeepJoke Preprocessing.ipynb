{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepJoke Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODOs:\n",
    "* Read reddit data\n",
    "* Clean data\n",
    "    * titles\n",
    "    * special characters\n",
    "* Tokenize\n",
    "* Pad sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "import codecs\n",
    "\n",
    "from tensorflow.contrib.keras import models\n",
    "\n",
    "pd.options.display.max_columns = 500\n",
    "pd.options.display.max_rows = 500\n",
    "pd.options.display.max_colwidth = 5000\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(194553, 4)\n"
     ]
    }
   ],
   "source": [
    "data_path = os.getcwd().replace(\"code\", \"joke-dataset\")\n",
    "reddit_data = pd.read_json(data_path + \"/reddit_jokes.json\")\n",
    "print(reddit_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def combine_title_body((title, body), verify_chars=15):\n",
    "    \"\"\"Given title and body:\n",
    "    - discard title if the first verify_chars chars of title is the same as that of body\n",
    "    - otherwise add title to body\"\"\"\n",
    "    title_lower = title.lower()\n",
    "    body_lower = body.lower()\n",
    "    if title_lower[0:verify_chars] == body_lower[0:verify_chars]:\n",
    "        combined = body\n",
    "    else:\n",
    "        combined = title + \" \" + body\n",
    "    return combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text = map(combine_title_body, zip(reddit_data[\"title\"].tolist(), reddit_data[\"body\"].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text_str = '\\n#\\n'.join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text_str = text_str.replace(\"\\n\\n\", \" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save output to text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with codecs.open(data_path + \"/reddit_jokes_processed.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(text_str)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
